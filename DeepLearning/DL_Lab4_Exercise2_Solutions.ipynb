{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slVkfZECGYuE"
      },
      "source": [
        "# Transfer Learning and Fine-Tuning\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "In this second exercise, we will learn how to apply fine-tuning on a model which is just an extension of transfer learning.\n",
        "\n",
        "This exercise is split in several parts:\n",
        "1.   Loading and Exploring Dataset\n",
        "2.   Preparing Dataset\n",
        "3.   Defining the Architecture of CNN\n",
        "4.   Training and Evaluation of the Model\n",
        "5.   Analysing the Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEKwxPVBF4Kd"
      },
      "source": [
        "# Predicting cats versus dogs (binary classification)\n",
        "\n",
        "## Dataset \n",
        "\n",
        "The dataset we will be using is the Cats vs Dogs.\n",
        "\n",
        "The data was originally shared by Microsoft in 2017.\n",
        "\n",
        "Each observation is a color image representing either a dog or a cat..\n",
        "\n",
        "The original dataset is avalaible here: [cats vs dogs](https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip)\n",
        "\n",
        "\n",
        "## Objective\n",
        "\n",
        "Our goal is to use a pre-trained Convolution Neural Network model and assess its performance then we will fine-tune it and see the impact. \n",
        "\n",
        "## Instructions\n",
        "\n",
        "This is a guided exercise where some of the code have already been pre-defined. Your task is to fill the remaining part of the code (it will be highlighted with placehoders) to train and evaluate your model.\n",
        "\n",
        "This exercise is split in several parts:\n",
        "1.   Loading and Exploration of the Dataset\n",
        "2.   Preparing the Dataset\n",
        "3.   Load a pre-trained VGG16 model\n",
        "4.   Training and Evaluation of the VGG16 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1uxFpFNZgj-"
      },
      "source": [
        "## Exercise 1 Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5TqsXRlhCcvO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%tensorflow_version` not found.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX32wNTnZn2M"
      },
      "source": [
        "### 1. Loading and exploring the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYx1x5OKCa2W"
      },
      "source": [
        "**[1.1]** \n",
        "First we need to download locally the zip file from the link provided earlier. We will be using the [get_file](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file) method from Tensorflow. The result of this method is the directory where the file has been downloaded.\n",
        "\n",
        "##### Task: import tensorflow and download the dataset locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rlRtw82Ca2W"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (3 lines of code)\n",
        "# Task: import tensorflow and download the dataset locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "b4YDJ6ztCa2a",
        "outputId": "608674dd-c921-4f9b-dd93-a69944a61d53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "68608000/68606236 [==============================] - 1s 0us/step\n",
            "68616192/68606236 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/root/.keras/datasets/cats_and_dogs_filtered.zip'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# SOLUTION\n",
        "import tensorflow as tf\n",
        "file_url = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "zip_dir = tf.keras.utils.get_file('cats_and_dogs_filtered.zip', origin=file_url, extract=True)\n",
        "zip_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68PZH0iXCa2c"
      },
      "source": [
        "**[1.2]** \n",
        "The method get_file() has already unzipped the file locally in the parent folder of the zip file.\n",
        "\n",
        "##### Task: Find the parent folder of the zip file and store it into variable called parent_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbNiSbQXCa2d"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (3 lines of code)\n",
        "# Task: Find the parent folder of the zip file and store it into variable called parent_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5NWXkj4gCa2f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'zip_dir' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\git\\azureml-samples\\DeepLearning\\DL_Lab4_Exercise2_Solutions.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/git/azureml-samples/DeepLearning/DL_Lab4_Exercise2_Solutions.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39m# SOLUTION\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/git/azureml-samples/DeepLearning/DL_Lab4_Exercise2_Solutions.ipynb#ch0000010?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpathlib\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/git/azureml-samples/DeepLearning/DL_Lab4_Exercise2_Solutions.ipynb#ch0000010?line=2'>3</a>\u001b[0m parent_dir \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPath(zip_dir)\u001b[39m.\u001b[39mparent\n",
            "\u001b[1;31mNameError\u001b[0m: name 'zip_dir' is not defined"
          ]
        }
      ],
      "source": [
        "# SOLUTION\n",
        "import pathlib\n",
        "parent_dir = pathlib.Path(zip_dir).parent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuQXi5eECa2i"
      },
      "source": [
        "**[1.3]** \n",
        "The images are already split into a 'train' and 'validation' folders under 'cats_and_dogs_filtered'\n",
        "\n",
        "##### Task: Create 2 variable called train_dir and test_dir that will contain the path to the 'train' and 'validation' folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mph8HnGzCa2j"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (2 lines of code)\n",
        "# Task: Create 2 variable called train_dir and test_dir that will contain the path to the 'train' and 'validation' folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FONBvEU5Ca2m"
      },
      "outputs": [],
      "source": [
        "train_dir = parent_dir / 'cats_and_dogs_filtered' / 'train'\n",
        "test_dir = parent_dir / 'cats_and_dogs_filtered' / 'validation'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TfftHwNCa2o"
      },
      "source": [
        "### 2.   Preparing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SqmBlqPCa2p"
      },
      "source": [
        "**[2.1]** We first need to create an [ImageDataGenerator()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator). This time we will not only normalise the images but we will also perform some data transformation: rescale=1./255, rotation_range=40, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest'\n",
        "\n",
        "##### Task: Import ImageDataGenerator and create an image generator for the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5L6Bwk9Ca2p"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (2 lines of code)\n",
        "# Task: Import ImageDataGenerator and create an image generator for the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeS9IGMJCa2r"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_img_gen = ImageDataGenerator(rescale=1./255, rotation_range=40, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFlC0sysCa2t"
      },
      "source": [
        "**[2.2]** Now we need to create the image data generator for the testing set\n",
        "\n",
        "##### Task: create an image generator for the testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMiljOxaCa2u"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: create an image generator for the testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk5NlNKwCa2w"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "test_img_gen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crFDtFLtCa2z"
      },
      "source": [
        "**[2.3]** Now we need to define the batch size for the data generators. For this dataset we will set it as 20.\n",
        "\n",
        "##### Task: create a variable containing the batch size value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2lUHZjcCa2z"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: create a variable containing the batch size value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pboe7vOZCa21"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "batch_size=20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCabTNcSCa23"
      },
      "source": [
        "**[2.4]** Now we can define our data generator by specifying its input stream. For this dataset, we will use the method [.flow_from_directory()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory) that reads images from folders directly.\n",
        "\n",
        "##### Task: create a data generator for the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7aVt4xSCa23"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: create a data generator for the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0acPRjeRCa25",
        "outputId": "0a528759-fd24-4a13-cceb-be0584a242f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# SOLUTION\n",
        "train_data_gen = train_img_gen.flow_from_directory(batch_size=batch_size, directory=train_dir, target_size=(100, 100), class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDlmUcxdCa27"
      },
      "source": [
        "**[2.5]** Now we can create the data generator for the testing set\n",
        "\n",
        "##### Task: create a data generator for the testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHNaXAUdCa28"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: create a data generator for the testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TslVuBC3Ca2-",
        "outputId": "86cc1da4-0e86-410c-d741-6c3aa173ed54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# SOLUTION\n",
        "test_data_gen = test_img_gen.flow_from_directory(batch_size=batch_size, directory=test_dir, target_size=(100, 100), class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoBUxuQgCa3B"
      },
      "source": [
        "**[2.5]** Let's create 2 variables containing the number of images for each set. This will be useful for fitting the model \n",
        "\n",
        "##### Task: create 2 variables called 'total_train' and 'total_val' that contain the number of images for the training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J6XKgdUCa3B"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (2 lines of code)\n",
        "# Task: create 2 variables called 'total_train' and 'total_val' that contain the number of images for the training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5YcuVVYCa3D"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "total_train = 2000\n",
        "total_val = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdpudNYVCa3F"
      },
      "source": [
        "### 3.   Defining the Architecture of CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHWSMlkYCa3F"
      },
      "source": [
        "**[3.1]** First we need to import the [VGG16](https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16) architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfedH9JyCa3G"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-wnUOPJCa3H"
      },
      "source": [
        "**[3.2]** Let's specify the dimensions of the input images\n",
        "\n",
        "##### Task: create a tuple called input_shape that will contains the height, width and number of channels of the input images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLrZ1266Ca3I"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: create a tuple called input_shape that will contains the height, width and number of channels of the input images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMyKqghOCa3J"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "input_shape = (100, 100, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un7uZl7-Ca3L"
      },
      "source": [
        "**[3.3]** Then we will instantiate this model by specifying the input image dimensions, the pre-trained weights from ImageNet and will not include the top layers as we are using a dataset with 2 classes (ImageNet has 200 classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqgP4xN0qte4"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Instantiate a pre-trained VGG16 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaj8aVGYCa3M",
        "outputId": "01dffe39-830a-4cb7-b10a-cd2abfb6971b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "base_model = VGG16(input_shape=input_shape, weights='imagenet', include_top=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZbUOWnuCa3z"
      },
      "source": [
        "**[3.4]** Now let's set a threshold that will define which layer we will train onward.\n",
        "\n",
        "###### Task: Create a variable called frozen_layers with the value 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNHfAvfgCa3z"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Create a variable called frozen_layers with the value 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW5AaoHgCa32"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "frozen_layers = 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAI1A6rLCa34"
      },
      "source": [
        "**[3.5]** We will set all the [layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers) up to the value in frozen_layers as non-trainable. By default all layers are trainable so we just need to \"freeze\" the layers we want.\n",
        "\n",
        "###### Task: Create a for loop that will iterate through the layers of base_model up to the value of frozen_layers and set each of them as non-trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edC49nk1Ca34"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (2 lines of code)\n",
        "# Task: Create a for loop that will iterate through the layers of base_model up to the value of frozen_layers and set each of them as non-trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbFVlcuhCa36"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "for layer in base_model.layers[:frozen_layers]:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2JeBPWkCa38"
      },
      "source": [
        "**[3.6]** Let's have a look at this model by displaying its summary with [.summary()](https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary)\n",
        "\n",
        "##### Task: Print the summary of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Vntf8zfCa38"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "##### Task: Print the summary of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5qV70k_Ca3-",
        "outputId": "eeeb38ed-f282-4006-ceac-1359f2c0718a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 100, 100, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 100, 100, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 100, 100, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 50, 50, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 50, 50, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 50, 50, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 25, 25, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 25, 25, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 25, 25, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 25, 25, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 12, 12, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 12, 12, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 6, 6, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 7,079,424\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# SOLUTION\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMe9uB4DCa4B"
      },
      "source": [
        "**[3.7]** Let's add 2 fully connected layers to this VGG16 model.\n",
        "\n",
        "##### Task: Create a new model by combining the VGG16 model to 2 fully connnected layers (the first one will have 500 units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcU9la0yCa4B"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (5 lines of code)\n",
        "# Task: Create a new model by combining the VGG16 model to 2 fully connnected layers (the first one will have 500 units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hpPlSMiCa4D"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "tuned_model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(500, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXMN04reCa4F"
      },
      "source": [
        "**[3.8]** We will now instantiate the [Adam()](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) class. We will specify the learning rate as 0.000001.\n",
        "\n",
        "##### Task: Create an Adam optimiser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KbFQfqnCa4F"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 lines of code)\n",
        "# Task: Create an Adam optimiser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th8tj4txCa4J"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "optimizer = tf.keras.optimizers.Adam(0.00001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5azxN6tCa4K"
      },
      "source": [
        "**[3.9]** Now that we have a defined architecture, we need to configure the learning process using the [.compile()](https://www.tensorflow.org/api_docs/python/tf/keras/Model#methods_2) method and specify the loss function, optimizer to be used and the metrics to be displayed. \n",
        "\n",
        "##### Task: Define the relevant loss function and metrics for the compile method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZbZzemyCa4K"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 lines of code)\n",
        "# Task: Define the relevant loss function and metrics for the compile method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRdMfF0sCa4M"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "tuned_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KkBBTdWCa4N"
      },
      "source": [
        "**[3.10]** Let's have a look at this model by displaying its summary with [.summary()](https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary)\n",
        "\n",
        "##### Task: Print the summary of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JWyFvYtCa4N"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "##### Task: Print the summary of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGzFnIvFCa4Q",
        "outputId": "1ca9de7a-0a8c-4c43-c81b-e022b1b809c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 3, 3, 512)         14714688  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4608)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 500)               2304500   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 501       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,019,689\n",
            "Trainable params: 9,384,425\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# SOLUTION\n",
        "tuned_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7aycMPUCa3j"
      },
      "source": [
        "### 4. Training and Evaluation of the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kTGe9KHfAmR"
      },
      "source": [
        "**[4.1]** Import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint modules from tensorflow.keras.callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOO3aQ0wf2Pm"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint modules from tensorflow.keras.callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp0SBCCGfAu7"
      },
      "outputs": [],
      "source": [
        "# Solution\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ4-bYBVf96M"
      },
      "source": [
        "**[4.2]** Instantiate an EarlyStopping callback that will stop the learning process if the model doesn't improve after 5 epochs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC2nIQZTf9_v"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Instantiate an EarlyStopping callback that will stop the learning process if the model doesn't improve after 5 epochs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAzsatD-f-IP"
      },
      "outputs": [],
      "source": [
        "# Solution\n",
        "early_stop_cb = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46jYZZ7rf_oW"
      },
      "source": [
        "**[4.3]** Instantiate a ReduceLROnPlateau callback that will decrease the learning rate by a factor of 0.2 if the model doesn't improve after 3 epochs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYyN53bSf_uT"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Instantiate a ReduceLROnPlateau callback that will decrease the learning rate by a factor of 0.2 if the model doesn't improve after 3 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pm6zB5Pof_xG"
      },
      "outputs": [],
      "source": [
        "# Solution\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0000001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeOuubbpgNyV"
      },
      "source": [
        "**[4.4]** Create a variable called `checkpoint_filepath` with the value './vgg_checkpoint'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxIePXSpgOAY"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Create a variable called checkpoint_filepath with the value './vgg_checkpoint'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eo1TunvgOGL"
      },
      "outputs": [],
      "source": [
        "# Solution\n",
        "checkpoint_filepath = './vgg_checkpoint'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpENmAGLgHxe"
      },
      "source": [
        "**[4.5]** Instantiate a ModelCheckpoint callback that will save the best model weights during the learning process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlB-itv6gH8H"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Instantiate a ModelCheckpoint callback that will save the best model weights during the learning process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIFLL4MlgH_j"
      },
      "outputs": [],
      "source": [
        "# Solution\n",
        "model_checkpoint_cb = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e01ZLXwCa3k"
      },
      "source": [
        "**[4.6]** Let's train our model. You can achieve this by calling the [.fit_generator()](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit_generator) method. This will take as input the data generators we created earlier. We need to also define the number of steps per epoch whoch corresponds the number of batches. We will run it for 5 epochs (the training will take some time).\n",
        "\n",
        "##### Task: Train the CNN model for 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1ageTUmCa3k"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Train the CNN model for 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz9iyXr5Ca3m",
        "outputId": "8bba06ac-7a3a-403c-a57d-eeb222ad7d92",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "100/100 [==============================] - 31s 194ms/step - loss: 0.5904 - accuracy: 0.6740 - val_loss: 0.4193 - val_accuracy: 0.8060 - lr: 1.0000e-05\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 19s 188ms/step - loss: 0.4066 - accuracy: 0.8225 - val_loss: 0.3162 - val_accuracy: 0.8560 - lr: 1.0000e-05\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 18s 184ms/step - loss: 0.3477 - accuracy: 0.8435 - val_loss: 0.2856 - val_accuracy: 0.8790 - lr: 1.0000e-05\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 18s 182ms/step - loss: 0.3307 - accuracy: 0.8580 - val_loss: 0.3025 - val_accuracy: 0.8740 - lr: 1.0000e-05\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 19s 186ms/step - loss: 0.3016 - accuracy: 0.8665 - val_loss: 0.2549 - val_accuracy: 0.8830 - lr: 1.0000e-05\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 18s 184ms/step - loss: 0.2859 - accuracy: 0.8745 - val_loss: 0.2572 - val_accuracy: 0.8830 - lr: 1.0000e-05\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 19s 186ms/step - loss: 0.2738 - accuracy: 0.8930 - val_loss: 0.2638 - val_accuracy: 0.8870 - lr: 1.0000e-05\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 18s 184ms/step - loss: 0.2408 - accuracy: 0.8950 - val_loss: 0.2719 - val_accuracy: 0.8870 - lr: 1.0000e-05\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 19s 192ms/step - loss: 0.2480 - accuracy: 0.8970 - val_loss: 0.2605 - val_accuracy: 0.9010 - lr: 1.0000e-05\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 19s 185ms/step - loss: 0.2219 - accuracy: 0.9055 - val_loss: 0.2710 - val_accuracy: 0.8970 - lr: 1.0000e-05\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 20s 199ms/step - loss: 0.2070 - accuracy: 0.9145 - val_loss: 0.2636 - val_accuracy: 0.8950 - lr: 2.0000e-06\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 18s 185ms/step - loss: 0.1861 - accuracy: 0.9285 - val_loss: 0.2423 - val_accuracy: 0.8960 - lr: 2.0000e-06\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 18s 182ms/step - loss: 0.1878 - accuracy: 0.9265 - val_loss: 0.2407 - val_accuracy: 0.8930 - lr: 2.0000e-06\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 18s 183ms/step - loss: 0.1879 - accuracy: 0.9240 - val_loss: 0.2323 - val_accuracy: 0.8960 - lr: 2.0000e-06\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 18s 181ms/step - loss: 0.1870 - accuracy: 0.9235 - val_loss: 0.2423 - val_accuracy: 0.8940 - lr: 2.0000e-06\n"
          ]
        }
      ],
      "source": [
        "# SOLUTION\n",
        "history = tuned_model.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=50,\n",
        "    validation_data=test_data_gen,\n",
        "    validation_steps=total_val // batch_size,\n",
        "    callbacks=[early_stop_cb, reduce_lr, model_checkpoint_cb]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4CgxmxeCa3q"
      },
      "source": [
        "### 5. Analysing the Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS-xryC2Ca3q"
      },
      "source": [
        "**[5.1]** Let's plot the learning curve for accuracy score on the training and validation sets. We will use the [.plot()](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot) method to create a line chart.\n",
        "\n",
        "##### Task: plot the accuracy for the training and validation set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4GHHNelCa3q"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (6 lines of code)\n",
        "# Task: plot the accuracy for the training and validation set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ygPgB2hqCa3s",
        "outputId": "297d778a-ff92-4790-c496-d7f35afad562"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# SOLUTION\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='training')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91BgOsDnCa4W"
      },
      "source": [
        "By unfreezing the last convolutional layers of the pre-trained VGG16 and training the model for 50 more epochs, we achieved even better accuracy. We have now 0.89 on the testing set.\n",
        "\n",
        "### Discussion: What are the reasons why fine-tuning helped to improve the accuracy score."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DL_Lab4_Exercise2_Solutions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
