{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slVkfZECGYuE"
      },
      "source": [
        "# Transfer Learning and Fine-Tuning\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "In this second exercise, we will learn how to apply fine-tuning on a model which is just an extension of transfer learning.\n",
        "\n",
        "This exercise is split in several parts:\n",
        "1.   Loading and Exploring Dataset\n",
        "2.   Preparing Dataset\n",
        "3.   Defining the Architecture of CNN\n",
        "4.   Training and Evaluation of the Model\n",
        "5.   Analysing the Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEKwxPVBF4Kd"
      },
      "source": [
        "# Predicting cats versus dogs (binary classification)\n",
        "\n",
        "## Dataset \n",
        "\n",
        "The dataset we will be using is the Cats vs Dogs.\n",
        "\n",
        "The data was originally shared by Microsoft in 2017.\n",
        "\n",
        "Each observation is a color image representing either a dog or a cat..\n",
        "\n",
        "The original dataset is avalaible here: [cats vs dogs](https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip)\n",
        "\n",
        "\n",
        "## Objective\n",
        "\n",
        "Our goal is to use a pre-trained Convolution Neural Network model and assess its performance then we will fine-tune it and see the impact. \n",
        "\n",
        "## Instructions\n",
        "\n",
        "This is a guided exercise where some of the code have already been pre-defined. Your task is to fill the remaining part of the code (it will be highlighted with placehoders) to train and evaluate your model.\n",
        "\n",
        "This exercise is split in several parts:\n",
        "1.   Loading and Exploration of the Dataset\n",
        "2.   Preparing the Dataset\n",
        "3.   Load a pre-trained VGG16 model\n",
        "4.   Training and Evaluation of the VGG16 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1uxFpFNZgj-"
      },
      "source": [
        "## Exercise 1 Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX32wNTnZn2M"
      },
      "source": [
        "### 1. Loading and exploring the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYx1x5OKCa2W"
      },
      "source": [
        "**[1.1]** \n",
        "First we need to download locally the zip file from the link provided earlier. We will be using the [get_file](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file) method from Tensorflow. The result of this method is the directory where the file has been downloaded.\n",
        "\n",
        "##### Task: import tensorflow and download the dataset locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "8rlRtw82Ca2W"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# Placeholder for student's code (3 lines of code)\n",
        "# Task: import tensorflow and download the dataset locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.8.0'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.version.VERSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Enable MLflow tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "mlflow.set_experiment('cats-and-dogs')\n",
        "mlflow.autolog()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data need to be present on the compute instance, or development machine. Refer to [explore_dataset.ipynb](explore_dataset.ipynb) for details on how to mount a registered `Dataset`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "b4YDJ6ztCa2a",
        "outputId": "608674dd-c921-4f9b-dd93-a69944a61d53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['train', 'validation', 'vectorize.py']\n"
          ]
        }
      ],
      "source": [
        "#zip_dir = os.path.expanduser('~/cloudfiles/data/cats_and_dogs_filtered')\n",
        "zip_dir = '/mnt/tmp/cats_dogs'\n",
        "import pathlib\n",
        "parent_dir = pathlib.Path(zip_dir)\n",
        "\n",
        "import os \n",
        "print(os.listdir(zip_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuQXi5eECa2i"
      },
      "source": [
        "**[1.3]** \n",
        "The images are already split into a 'train' and 'validation' folders under 'cats_and_dogs_filtered'\n",
        "\n",
        "##### Task: Create 2 variable called train_dir and test_dir that will contain the path to the 'train' and 'validation' folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Mph8HnGzCa2j"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (2 lines of code)\n",
        "# Task: Create 2 variable called train_dir and test_dir that will contain the path to the 'train' and 'validation' folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "FONBvEU5Ca2m"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/azureuser/cloudfiles/data/cats_and_dogs_filtered/train\n",
            "/home/azureuser/cloudfiles/data/cats_and_dogs_filtered/validation\n"
          ]
        }
      ],
      "source": [
        "train_dir = parent_dir / 'train'\n",
        "test_dir = parent_dir / 'validation'\n",
        "\n",
        "print(train_dir)\n",
        "print(test_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TfftHwNCa2o"
      },
      "source": [
        "### 2.   Preparing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SqmBlqPCa2p"
      },
      "source": [
        "**[2.1]** We first need to create an [ImageDataGenerator()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator). This time we will not only normalise the images but we will also perform some data transformation: rescale=1./255, rotation_range=40, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest'\n",
        "\n",
        "##### Task: Import ImageDataGenerator and create an image generator for the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "P5L6Bwk9Ca2p"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (2 lines of code)\n",
        "# Task: Import ImageDataGenerator and create an image generator for the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "yeS9IGMJCa2r"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_img_gen = ImageDataGenerator(rescale=1./255, rotation_range=40, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFlC0sysCa2t"
      },
      "source": [
        "**[2.2]** Now we need to create the image data generator for the testing set\n",
        "\n",
        "##### Task: create an image generator for the testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "HMiljOxaCa2u"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: create an image generator for the testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "mk5NlNKwCa2w"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "test_img_gen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crFDtFLtCa2z"
      },
      "source": [
        "**[2.3]** Now we need to define the batch size for the data generators. For this dataset we will set it as 20.\n",
        "\n",
        "##### Task: create a variable containing the batch size value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "b2lUHZjcCa2z"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: create a variable containing the batch size value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "pboe7vOZCa21"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "batch_size=20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCabTNcSCa23"
      },
      "source": [
        "**[2.4]** Now we can define our data generator by specifying its input stream. For this dataset, we will use the method [.flow_from_directory()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory) that reads images from folders directly.\n",
        "\n",
        "##### Task: create a data generator for the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "i7aVt4xSCa23"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: create a data generator for the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0acPRjeRCa25",
        "outputId": "0a528759-fd24-4a13-cceb-be0584a242f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# SOLUTION\n",
        "train_data_gen = train_img_gen.flow_from_directory(batch_size=batch_size, directory=train_dir, target_size=(100, 100), class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDlmUcxdCa27"
      },
      "source": [
        "**[2.5]** Now we can create the data generator for the testing set\n",
        "\n",
        "##### Task: create a data generator for the testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "DHNaXAUdCa28"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: create a data generator for the testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TslVuBC3Ca2-",
        "outputId": "86cc1da4-0e86-410c-d741-6c3aa173ed54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# SOLUTION\n",
        "test_data_gen = test_img_gen.flow_from_directory(batch_size=batch_size, directory=test_dir, target_size=(100, 100), class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoBUxuQgCa3B"
      },
      "source": [
        "**[2.5]** Let's create 2 variables containing the number of images for each set. This will be useful for fitting the model \n",
        "\n",
        "##### Task: create 2 variables called 'total_train' and 'total_val' that contain the number of images for the training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'cats': 0, 'dogs': 1}"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data_gen.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "_J6XKgdUCa3B"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (2 lines of code)\n",
        "# Task: create 2 variables called 'total_train' and 'total_val' that contain the number of images for the training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "a5YcuVVYCa3D"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "total_train = 2000\n",
        "total_val = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdpudNYVCa3F"
      },
      "source": [
        "### 3.   Defining the Architecture of CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHWSMlkYCa3F"
      },
      "source": [
        "**[3.1]** First we need to import the [VGG16](https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16) architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "YfedH9JyCa3G"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-wnUOPJCa3H"
      },
      "source": [
        "**[3.2]** Let's specify the dimensions of the input images\n",
        "\n",
        "##### Task: create a tuple called input_shape that will contains the height, width and number of channels of the input images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "DLrZ1266Ca3I"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: create a tuple called input_shape that will contains the height, width and number of channels of the input images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "pMyKqghOCa3J"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "input_shape = (100, 100, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un7uZl7-Ca3L"
      },
      "source": [
        "**[3.3]** Then we will instantiate this model by specifying the input image dimensions, the pre-trained weights from ImageNet and will not include the top layers as we are using a dataset with 2 classes (ImageNet has 200 classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "YqgP4xN0qte4"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Instantiate a pre-trained VGG16 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaj8aVGYCa3M",
        "outputId": "01dffe39-830a-4cb7-b10a-cd2abfb6971b"
      },
      "outputs": [],
      "source": [
        "base_model = VGG16(input_shape=input_shape, weights='imagenet', include_top=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZbUOWnuCa3z"
      },
      "source": [
        "**[3.4]** Now let's set a threshold that will define which layer we will train onward.\n",
        "\n",
        "###### Task: Create a variable called frozen_layers with the value 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "HNHfAvfgCa3z"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Create a variable called frozen_layers with the value 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "KW5AaoHgCa32"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "frozen_layers = 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAI1A6rLCa34"
      },
      "source": [
        "**[3.5]** We will set all the [layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers) up to the value in frozen_layers as non-trainable. By default all layers are trainable so we just need to \"freeze\" the layers we want.\n",
        "\n",
        "###### Task: Create a for loop that will iterate through the layers of base_model up to the value of frozen_layers and set each of them as non-trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "edC49nk1Ca34"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (2 lines of code)\n",
        "# Task: Create a for loop that will iterate through the layers of base_model up to the value of frozen_layers and set each of them as non-trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "JbFVlcuhCa36"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "for layer in base_model.layers[:frozen_layers]:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2JeBPWkCa38"
      },
      "source": [
        "**[3.6]** Let's have a look at this model by displaying its summary with [.summary()](https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary)\n",
        "\n",
        "##### Task: Print the summary of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "0Vntf8zfCa38"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "##### Task: Print the summary of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5qV70k_Ca3-",
        "outputId": "eeeb38ed-f282-4006-ceac-1359f2c0718a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 100, 100, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 100, 100, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 100, 100, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 50, 50, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 50, 50, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 50, 50, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 25, 25, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 25, 25, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 25, 25, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 25, 25, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 12, 12, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 12, 12, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 6, 6, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 7,079,424\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# SOLUTION\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMe9uB4DCa4B"
      },
      "source": [
        "**[3.7]** Let's add 2 fully connected layers to this VGG16 model.\n",
        "\n",
        "##### Task: Create a new model by combining the VGG16 model to 2 fully connnected layers (the first one will have 500 units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "ZcU9la0yCa4B"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (5 lines of code)\n",
        "# Task: Create a new model by combining the VGG16 model to 2 fully connnected layers (the first one will have 500 units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "4hpPlSMiCa4D"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "tuned_model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(500, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXMN04reCa4F"
      },
      "source": [
        "**[3.8]** We will now instantiate the [Adam()](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) class. We will specify the learning rate as 0.000001.\n",
        "\n",
        "##### Task: Create an Adam optimiser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "3KbFQfqnCa4F"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 lines of code)\n",
        "# Task: Create an Adam optimiser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "th8tj4txCa4J"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "optimizer = tf.keras.optimizers.Adam(0.00001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5azxN6tCa4K"
      },
      "source": [
        "**[3.9]** Now that we have a defined architecture, we need to configure the learning process using the [.compile()](https://www.tensorflow.org/api_docs/python/tf/keras/Model#methods_2) method and specify the loss function, optimizer to be used and the metrics to be displayed. \n",
        "\n",
        "##### Task: Define the relevant loss function and metrics for the compile method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "RZbZzemyCa4K"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 lines of code)\n",
        "# Task: Define the relevant loss function and metrics for the compile method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "cRdMfF0sCa4M"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "tuned_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KkBBTdWCa4N"
      },
      "source": [
        "**[3.10]** Let's have a look at this model by displaying its summary with [.summary()](https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary)\n",
        "\n",
        "##### Task: Print the summary of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "_JWyFvYtCa4N"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "##### Task: Print the summary of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGzFnIvFCa4Q",
        "outputId": "1ca9de7a-0a8c-4c43-c81b-e022b1b809c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 3, 3, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 500)               2304500   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 501       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,019,689\n",
            "Trainable params: 9,384,425\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# SOLUTION\n",
        "tuned_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7aycMPUCa3j"
      },
      "source": [
        "### 4. Training and Evaluation of the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kTGe9KHfAmR"
      },
      "source": [
        "**[4.1]** Import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint modules from tensorflow.keras.callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "UOO3aQ0wf2Pm"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint modules from tensorflow.keras.callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "Wp0SBCCGfAu7"
      },
      "outputs": [],
      "source": [
        "# Solution\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ4-bYBVf96M"
      },
      "source": [
        "**[4.2]** Instantiate an EarlyStopping callback that will stop the learning process if the model doesn't improve after 5 epochs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "cC2nIQZTf9_v"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Instantiate an EarlyStopping callback that will stop the learning process if the model doesn't improve after 5 epochs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "pAzsatD-f-IP"
      },
      "outputs": [],
      "source": [
        "# Solution\n",
        "early_stop_cb = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46jYZZ7rf_oW"
      },
      "source": [
        "**[4.3]** Instantiate a ReduceLROnPlateau callback that will decrease the learning rate by a factor of 0.2 if the model doesn't improve after 3 epochs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "RYyN53bSf_uT"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Instantiate a ReduceLROnPlateau callback that will decrease the learning rate by a factor of 0.2 if the model doesn't improve after 3 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Pm6zB5Pof_xG"
      },
      "outputs": [],
      "source": [
        "# Solution\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0000001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeOuubbpgNyV"
      },
      "source": [
        "**[4.4]** Create a variable called `checkpoint_filepath` with the value './vgg_checkpoint'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "dxIePXSpgOAY"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Create a variable called checkpoint_filepath with the value './vgg_checkpoint'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "1eo1TunvgOGL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/azureuser/cloudfiles/data/model_checkpoint/vgg_checkpoint\n"
          ]
        }
      ],
      "source": [
        "# Solution\n",
        "checkpoint_filepath = os.path.expanduser('~/cloudfiles/data/model_checkpoint/vgg_checkpoint')\n",
        "print(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpENmAGLgHxe"
      },
      "source": [
        "**[4.5]** Instantiate a ModelCheckpoint callback that will save the best model weights during the learning process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "UlB-itv6gH8H"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Instantiate a ModelCheckpoint callback that will save the best model weights during the learning process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "cIFLL4MlgH_j"
      },
      "outputs": [],
      "source": [
        "# Solution\n",
        "model_checkpoint_cb = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e01ZLXwCa3k"
      },
      "source": [
        "**[4.6]** Let's train our model. You can achieve this by calling the [.fit_generator()](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit_generator) method. This will take as input the data generators we created earlier. We need to also define the number of steps per epoch whoch corresponds the number of batches. We will run it for 5 epochs (the training will take some time).\n",
        "\n",
        "##### Task: Train the CNN model for 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "k1ageTUmCa3k"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (1 line of code)\n",
        "# Task: Train the CNN model for 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz9iyXr5Ca3m",
        "outputId": "8bba06ac-7a3a-403c-a57d-eeb222ad7d92",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-56-07d7bd374a47>:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = tuned_model.fit_generator(\n",
            "2022/04/12 13:18:16 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'a86034b2-a0f0-492d-a5b8-b51c8e4e160d', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current tensorflow workflow\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "100/100 [==============================] - 39s 162ms/step - loss: 0.5555 - accuracy: 0.7145 - val_loss: 0.4326 - val_accuracy: 0.7890 - lr: 1.0000e-05\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 24s 245ms/step - loss: 0.3976 - accuracy: 0.8290 - val_loss: 0.3213 - val_accuracy: 0.8500 - lr: 1.0000e-05\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 30s 300ms/step - loss: 0.3429 - accuracy: 0.8490 - val_loss: 0.3245 - val_accuracy: 0.8550 - lr: 1.0000e-05\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 45s 456ms/step - loss: 0.3250 - accuracy: 0.8570 - val_loss: 0.2864 - val_accuracy: 0.8720 - lr: 1.0000e-05\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 18s 180ms/step - loss: 0.2863 - accuracy: 0.8840 - val_loss: 0.2795 - val_accuracy: 0.8740 - lr: 1.0000e-05\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 33s 327ms/step - loss: 0.2726 - accuracy: 0.8815 - val_loss: 0.2617 - val_accuracy: 0.8780 - lr: 1.0000e-05\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 23s 233ms/step - loss: 0.2649 - accuracy: 0.8860 - val_loss: 0.2719 - val_accuracy: 0.8910 - lr: 1.0000e-05\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 14s 144ms/step - loss: 0.2460 - accuracy: 0.8975 - val_loss: 0.2839 - val_accuracy: 0.8860 - lr: 1.0000e-05\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 18s 176ms/step - loss: 0.2383 - accuracy: 0.8985 - val_loss: 0.2557 - val_accuracy: 0.8960 - lr: 1.0000e-05\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 13s 131ms/step - loss: 0.2231 - accuracy: 0.9005 - val_loss: 0.2588 - val_accuracy: 0.8950 - lr: 1.0000e-05\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 19s 189ms/step - loss: 0.2107 - accuracy: 0.9145 - val_loss: 0.2352 - val_accuracy: 0.8980 - lr: 1.0000e-05\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 13s 133ms/step - loss: 0.2066 - accuracy: 0.9155 - val_loss: 0.2802 - val_accuracy: 0.8960 - lr: 1.0000e-05\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 14s 138ms/step - loss: 0.1952 - accuracy: 0.9200 - val_loss: 0.3030 - val_accuracy: 0.8830 - lr: 1.0000e-05\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 13s 135ms/step - loss: 0.2023 - accuracy: 0.9190 - val_loss: 0.2860 - val_accuracy: 0.8840 - lr: 1.0000e-05\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 25s 247ms/step - loss: 0.1606 - accuracy: 0.9380 - val_loss: 0.2343 - val_accuracy: 0.9040 - lr: 2.0000e-06\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 13s 127ms/step - loss: 0.1731 - accuracy: 0.9365 - val_loss: 0.2262 - val_accuracy: 0.9040 - lr: 2.0000e-06\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 14s 143ms/step - loss: 0.1553 - accuracy: 0.9355 - val_loss: 0.2388 - val_accuracy: 0.9130 - lr: 2.0000e-06\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 13s 131ms/step - loss: 0.1475 - accuracy: 0.9410 - val_loss: 0.2668 - val_accuracy: 0.9020 - lr: 2.0000e-06\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 14s 139ms/step - loss: 0.1625 - accuracy: 0.9305 - val_loss: 0.2395 - val_accuracy: 0.9090 - lr: 2.0000e-06\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - 15s 147ms/step - loss: 0.1378 - accuracy: 0.9505 - val_loss: 0.2506 - val_accuracy: 0.9080 - lr: 4.0000e-07\n",
            "Epoch 21/50\n",
            "100/100 [==============================] - 15s 147ms/step - loss: 0.1409 - accuracy: 0.9515 - val_loss: 0.2463 - val_accuracy: 0.9080 - lr: 4.0000e-07\n",
            "Epoch 22/50\n",
            "100/100 [==============================] - 15s 145ms/step - loss: 0.1417 - accuracy: 0.9440 - val_loss: 0.2395 - val_accuracy: 0.9070 - lr: 4.0000e-07\n",
            "Epoch 23/50\n",
            "100/100 [==============================] - 14s 139ms/step - loss: 0.1445 - accuracy: 0.9450 - val_loss: 0.2473 - val_accuracy: 0.9080 - lr: 1.0000e-07\n",
            "Epoch 24/50\n",
            "100/100 [==============================] - 14s 144ms/step - loss: 0.1540 - accuracy: 0.9410 - val_loss: 0.2461 - val_accuracy: 0.9080 - lr: 1.0000e-07\n",
            "Epoch 25/50\n",
            "100/100 [==============================] - 13s 130ms/step - loss: 0.1338 - accuracy: 0.9530 - val_loss: 0.2443 - val_accuracy: 0.9090 - lr: 1.0000e-07\n",
            "Epoch 26/50\n",
            "100/100 [==============================] - 13s 134ms/step - loss: 0.1403 - accuracy: 0.9480 - val_loss: 0.2505 - val_accuracy: 0.9090 - lr: 1.0000e-07\n",
            "Epoch 27/50\n",
            "100/100 [==============================] - 13s 128ms/step - loss: 0.1357 - accuracy: 0.9510 - val_loss: 0.2508 - val_accuracy: 0.9090 - lr: 1.0000e-07\n",
            "Epoch 28/50\n",
            "100/100 [==============================] - 13s 131ms/step - loss: 0.1500 - accuracy: 0.9465 - val_loss: 0.2443 - val_accuracy: 0.9080 - lr: 1.0000e-07\n",
            "Epoch 29/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.1487 - accuracy: 0.9450 - val_loss: 0.2471 - val_accuracy: 0.9080 - lr: 1.0000e-07\n",
            "Epoch 30/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.1395 - accuracy: 0.9500 - val_loss: 0.2463 - val_accuracy: 0.9080 - lr: 1.0000e-07\n",
            "INFO:tensorflow:Assets written to: /tmp/tmpe0tyy80_/model/data/model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022/04/12 13:28:48 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpe0tyy80_/model, flavor: keras), fall back to return ['tensorflow==2.8.0', 'keras==2.8.0']. Set logging level to DEBUG to see the full traceback.\n"
          ]
        }
      ],
      "source": [
        "# SOLUTION\n",
        "history = tuned_model.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=50,\n",
        "    validation_data=test_data_gen,\n",
        "    validation_steps=total_val // batch_size,\n",
        "    callbacks=[early_stop_cb, reduce_lr, model_checkpoint_cb]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4CgxmxeCa3q"
      },
      "source": [
        "### 5. Analysing the Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS-xryC2Ca3q"
      },
      "source": [
        "**[5.1]** Let's plot the learning curve for accuracy score on the training and validation sets. We will use the [.plot()](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot) method to create a line chart.\n",
        "\n",
        "##### Task: plot the accuracy for the training and validation set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "L4GHHNelCa3q"
      },
      "outputs": [],
      "source": [
        "# Placeholder for student's code (6 lines of code)\n",
        "# Task: plot the accuracy for the training and validation set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ygPgB2hqCa3s",
        "outputId": "297d778a-ff92-4790-c496-d7f35afad562"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEGCAYAAACQO2mwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAv6ElEQVR4nO3dd5yU9dX4/c/Z3mALu5RdelFAUJC1N5RYYgNblIiKJUR/0dvb3MmjRqNG70Sf/BIfTWxRg6AmdiVosGAJdmHpVaSzjbaVbbM7c54/rguYLbPMws7O7s55v17z2rnazLl2YM5+u6gqxhhjTEuiwh2AMcaYzsuShDHGmIAsSRhjjAnIkoQxxpiALEkYY4wJKCbcAbSXzMxMHTx4cLjDMMaYLmXx4sW7VTUr0PFukyQGDx5MXl5euMMwxpguRUS2tnbcqpuMMcYEZEnCGGNMQJYkjDHGBGRJwhhjTECWJIwxxgRkScIYY0xAliSMMcYEZEnCGBMxajxeXvhqM6sKysMdSpcR0sF0InIe8DgQDTyvqo80OT4ImAlkASXANFXNd495gZXuqdtU9eJQxmqMabu6Bi+lVfWUVHmoqW/gmP5pxER3zr89K2vruXFWHgu3lAAwNieVq44fwMXHZNMjITbM0XVeEqpFh0QkGlgPnA3kA4uAqaq6xu+cN4D3VHW2iJwFXK+q17jH9qpqSrDvl5ubqzbi2pj2tXl3FfNWFlFS5aG0ykNJ9YGfJXs9VHm8jc4/sk8PHrj4KE4a1itMEbestMrDdS8sZE1hBX+4dCzVdQ28umg764orSYqL5sKj+zH1+IGMG5CGiBz09VSVTburyNtSwsLNpazfUclxgzOYMj6bsTmpQb1Ge/H5lNJqD71S4g/pehFZrKq5AY+HMEmcBDygque623cDqOrDfuesBs5T1e3i/FbLVbWne8yShDFhtLeugXMeXUBheS2JsdFkJMeRkRxHenIcGUmxZCTHk5Ec627HUe3x8uj89RSU1XDh0f2454JR9EtNDPdtsLOilml//44te6p5+upjmTSqD+B80S/bXsarC7fz7opCqj1eRvbtwVXHDeCS8f1JTTpQumjw+lhTVMHCzSXkbSklb2sJu/d6AMhIjmN4VgrLtpfh8foYmpnM5HE5TB6XzeDM5JDe1xuL83lt0Xay0xJ4dcZJh/Q64UwSl+MkgJvc7WuAE1T1Vr9z/gl8p6qPi8ilwFtApqruEZEGYBnQADyiqnNaeI8ZwAyAgQMHTti6tdUpSIwxbfC7d1cz6+stvP7zkzhucEZQ19R4vDyzYCPPLNhIlAi3TRrOjacOIT4mOsTRtiy/tJppz3/Hzso6nr82l5OHZ7Z4XmVtPe8uL+LVRdtYkV9OfEwU54/tx6BeSeRtKWXJtlKq3VLTgIxEjhucsf8xLCsZEaG8up73VxUxZ1kB320uQRXGDUhj8rhsLjw6m6weh/aXvj+vT1mwfievLNzOp+t24vUpJw7NYOrxA7n4mOxDKsF09iSRDTwBDAE+By4DxqhqmYjkqGqBiAwFPgUmqerGQO9nJQlj2s+y7WVc8tRXTDthEA9NGdPm67eXVPPQe2v4aM0OhmQmc99FoznzyN4hiDSwTbv2cvXz31FV18CsG47n2IHpQV23qqCc1xZtZ87SAvZ6GhjZtyfHDU7fnxT6piYc9DUKy2p4d3khc5YVsraogugo4ZThmUwZl80ZR2SRkRzXpi/0grIaXlu0nTfytlNUXktmShyXTejPVccNZMhhllY6dXVTk/NTgHWq2r+FY7Nw2i7eDPR+liSMaR/1Xh8XP/EVJVV1zP/lGfQ8jEbdBet38bu5q9m0u4ofjerDfReOZmCvpHaMtmVriyq45u/foQov3ng8R2Wntvk1auu91Ht9h92ovX5HJXOWFvCvZYUUlNUAkBQXTXZaItlpieSkJZDjPne2E/cnok/W7uTVRdtYsH4XAKeNyGLqcQOYNKoPcTHt00EgnEkiBqfhehJQgNNw/VNVXe13TiZQoqo+Efk94FXV+0QkHahW1Tr3nG+Ayf6N3k1ZkjCmfTyzYCOPvL+OZ6ZN4LwxfQ/79TwNPmZ+tZm/fPIDDT7l5tOH8tMTBlFZ6/SKKvFvEK+qp6SqjpLqekqrPFTVNTBhUDrnHNWX00ZkkhB78GqrpdtKuW7mQpLjY3j5phMYlhV002ZI+XzKkm2lLM8vp6C0hsKyGgrLaygorWFPlafRuSKQGBtNtcdL354J/CS3P1fkDmBARvsn2LAlCffNzwcew+kCO1NVfy8iDwJ5qjrXrZJ6GFCc6qZfuInhZOBvgA9nLMdjqvr31t7LkoQxh2/rnirOfexzTh+RxbPXBvzeOCTF5bU8/P5a/rWsMOA5KfExpCe7jeJJscRER/Htpj1U1jaQFBfNGUdkce5RfTlzZG9SE5v/hf/Nxj3cNHsRvVLi+cdNJ4TkSzUUauu9TtIoq6WgrJqCslpKqzxMPDKLM47ICmm34rAmiY5kScKYw6OqXDtzIUu3lTH/l6eHrGfS4q2lrC4sJz0pbn+PqYzkONKSYlts4PY0+Ph20x4+XF3M/DU72FlZR0yUcNKwXpxzVF/OGd2HPj0T+GzdTm5+eTEDM5J4+aYT6NPz4G0HxpKEMRGhtt7LeyuKALjs2JxD6uXyztJ87nhtOQ9OPoprTxrczhG2D59PWbq9jI/WFPPR6h1s3l0FwDH9U1ldWMHIfj148YYTyEiOC3OkXcfBkkS3Wb7UmEi0o6KWl7/dyj+/27a/Xnvh5j08NGVMm7qdllR5eOi9tYwfmMbVJwwKVbiHLSpKmDAonQmD0rnrvJH8sHMvH64qZv7aHZw5sjd//skxh9XQbpqzJGFMO6qt9xIXHUVUVGhH3C7fXsYLX23mvRVFeFU568jeXH/KEBZu3sNfPt3A5t1VPD1tAplBjsL9/b/XUlFTz8OXjiU6xLG3FxHhiD49OKJPD26bNCLc4XRbliSMaScr8su4/JlvQKFfk26N/fc/TyA7LTGoXjpN1Xt9fLi6mJlfbmbJtjJS4mOYduIgpp88eP/I3lNHZDKiTw9+9cZyJj/xFX+fnsvIvj1bfd2vNuzmrSX5/OLMYQc910Qea5Mwph3UNXi56K9fUl5Tz5RxORSU1ezvrbKjspam/80yU+Lo0zPBmebCrwF33xQX6cmx9EqOJz05FkF4Y/F2XvpmK0XltQzqlcR1Jw3mitz+Afvwr8gv42cv5rG3toHHrhrP2aP7tHhebb2Xcx/7nCgR3r/9tENKXqZrszYJYzrAXz75gfU79jJzei5njWz8hexp8LGjonZ/4igodfrH76ioo6TKw7aSakqqPFTWNrT6HicP68WDk8dw1sjeB60SOrp/GnNvPZUZL+Yx46U8fn3ukdxyxrBmDdqPf/IDW/dU88+fnWAJwrTIkoQxh2lFfhnPLNjEZcf2b5YgAOJiohiQkXTQPvueBh9l1e4Mq1UedwruOirrGjhrZO82VwX16ZnAaz8/iV+/uYI/fvA964sreeSyo/cng7VFFTz7+SaumNCfk4e1PKeRMZYkjDkMdQ1efvXGcjJT4rjvotGH9VpxMVH07plA73bs358QG81frhrHkX1S+NNH69myp5pnr5lAr5R47np7JWmJsfzm/FHt9n6m+7EkYcxh8K9mamkEcGcgItx61giG9+7BHa8t4+InvuK8MX1Zvr2Mx68aR7qNKTCt6JxLSBnTBRysmqmzOW9MX9685SSiBGZ9vYXTj8ji4mOywx2W6eSsJGHMIWjPaqaOdFR2Kv+69VSe/3IT008e3KErqJmuyZKEMYfgr59s6PTVTIFk9Yjn7h9bO4QJjlU3GdNGK/LLeHrBxi5TzWTM4bAkYUwbdNVqJmMOlVU3GdMGXbmayZhDYSUJY4Jk1UwmElmSMCYIVs1kIpVVN5mIVVReQ0xUFOnuMpmtsWomE6ksSZiIUlRew9xlhcxZVsjaoor9+1MTY90ZWWObzcwaEx1l1UwmYlmSMN1eeU09768sYs6yAr7bXIIqHDMgjXvOH0V8bJQ7mZ6Hkup6Sqs8FJTVsqqggpIqDx6vD4CctETuu9CqmUzksSRhuqXaei+frdvJnGUFfLZuFx6vjyGZydw+aQSTx+UwxF2kpzWqSrXHS0mVh8yUeBLjbCptE3ksSZguS1WpqGnwm1rbmWZ78ZZS5q0qorK2gcyUeK4+cSBTxuVwdP/UNk1DISIkx8eQHG//TUzksn/9ptOr8Xj566c/sHl3lZMMqj2UVNVTWu3B62u+smJyXDTnjunLlHE5nDys10EbpY0xgVmSMJ1ag9fHba8s4ZN1OxmWlUJGUhxDM1OYMCiOjORY0pPi6JXSuKE5q0c88THdpGrI54Mtn8OAEyA2MdzRmAhkScJ0WqrKPe+s4uO1O3lo8lFcc9LgcIfUscoL4J2fw5YvoP9xMPVVSLYV5EzHsnK46bT+/NF6Xsvbzn+dNTzyEsTqOfD0yVCwBE6+DYpXwvOTYPeGcEdmIoyVJEynNOurzTzx2QamHj+AO84+ItzhdJy6Snj/Llj2MuRMgEufg17DYPQU+OeV8PcfwVWvwKCTDu99fF6QKIjE9STqa0F97fuaIhCT0C1/n5YkTKfz3opCfvfeGs4e3YeHJo+JnIVx8vPgrZugbCuc/ms4406Idkd398+Fm+bDP66AFy+GS56BMZe1/T2qS+CLP8PC55ztpF7uI+PA8+TMxvviUoAgPgMBEtKcaxJSD/0LUxU8e6FqN9SUOtvBqK+G6j2BH1Xuz4aaQ4vrYKLj3d9fL7/fawuPuGSC+n22RVwS9A7NGiGWJEyn8vWG3fzyteXkDkrnr1PHR0bPJJ8XvngU/vMw9MyB6f+GQSc3Py9jKNw4H169Gt68Acq2wSn/HdyXcUOdkxg+/79QWw5HXwk9+rhfoCXOz6Llzs/assO/p6gYSMxonnCS3O2oaCcJBPpC93oOP4b41APvm9IXeh/lbCemO/G1J/U6v9cqv3so295+v8+DycmFn30Skpe2JGE6jVUF5cx4aTGDM5N4/trjSIjtJj2UWlO61Wmc3vYNjL0CLviz81d4IEkZcO0cmPN/4OMHoHQLnP9niA7wX1kVVr8NH//OKaEMmwRnPwh9xwR+D2+D8xf8vi87T1Vw97Lvi7J6T5MEUAI71x54jl/JICHtQCJJGwjZ4w4kkqRebftCj4k/8FqJGRATF9x1oeatb/L7rG7/92jt38xhsiRhOoVte6qZ/sIiUhNjefGGE0hNioBJ9Fa8Dv/+H+f5pc/B0T8J7rqYeOf89EFO1VF5PlwxC+J7ND5v69fw0b1QsBj6jIFpb8PwSQd//egYSMlyHu3N5yYSX4PzRR4ouXUn0bGQ0tt5dEER8AmZzm5XZR3XzPyOBp+PV284kb6pCeEO6dCU5ztVQXt3HPxc9TnnDTgRLv0bpA9u23tFRcGk+yBtELx3B8z8MVz9OvTMdnpAfXw/rHsPemTD5KfgmKucKp5wi4p2SkOmywhpkhCR84DHgWjgeVV9pMnxQcBMIAsoAaapar577DrgXvfU/1XV2aGM1YTH3roGrp+1kB0VtfzzZycyvHdKuEM6NKpOqWD3eqdBOZh2gt6j4bifHd5f0xOug9QceH06PDcJRpwNy/7h9LQ561448RdOo6YxhyhkSUJEooEngbOBfGCRiMxV1TV+p/0JeFFVZ4vIWcDDwDUikgHcD+TiVGAudq8tDVW8puN5Gnzc/NJi1hZV8ty1Ezh2YHq4Qzp0a/4F6z+Ac/7XGdfQkYb/CG54H/7xE1j6MkyYDhPv6rLVG6ZzCWVJ4nhgg6puAhCRV4HJgH+SGA380n3+GTDHfX4uMF9VS9xr5wPnAa+EMF4TIj6fUlRRy8ade9mwcy8bdzmPH3bsZU+Vhz9dcUzXXqehpgze/3+g3zFwwi3hiaHvWLjlK2ecRfqg8MRguqVQJokcYLvfdj5wQpNzlgOX4lRJXQL0EJFeAa7NafoGIjIDmAEwcODAdgvctE2910dptYfSqnr2VNWxZ6+HLbur2OAmg027qqj2ePefn5oYy/DeKUwa1Zszj+zNj8f2C2P07eDjB6BqF/z09fA2xCZlWH2/aXfhbrj+FfCEiEwHPgcKAG+rV/hR1WeBZwFyc3ODHHFjDsXK/HLeW1FISZUzLXdJtTM1954qD5W1DS1ek5OWyLDeKRw3OIPhvVMYlpXC8N4p9EqOO/gAOW8DFC+Hrd843UOLljtdG7OOhMwRkHkkZB7hjB0IZ1fHrd/A4hfgpFud7pvGdDOhTBIFwAC/7f7uvv1UtRCnJIGIpACXqWqZiBQAE5tc+58QxmpaMX/NDm795xJUITMljnR3ttUB6UkHlvpMiSMjKY705Fj61W6kb0o0iam9/UaYHoSnyhlxvO0bp+tmfh7Uu/3z0wc7E9zVlMKWL2HFaweuk2jIGOImjRFOEhl0ctt7Cx2Khjp493ZIHQhn/ib072dMGIQySSwCRojIEJzkcBXwU/8TRCQTKFFVH3A3Tk8ngA+BP4jIvpbMc9zjpoO9tmgbd7+9krH903hh+nFkJLfyV3uDBz68GxY933h/TGLgqR9qyw+UFHwNgDh9+sdfDQNPgoEnOt06/dVVwp4NsGu905to3+OHj8BXD7HJcM3bzrWh9OVjsPt7uPrN4BKhMV1QyJKEqjaIyK04X/jRwExVXS0iDwJ5qjoXp7TwsIgoTnXTL9xrS0TkIZxEA/DgvkZs0zFUlaf+s5H/++H3nH5EFk9ffWzrK7RVFMHr10L+Qqfb5eBTobrJqNt9I3FLtzjbdeXOfDc5E+CU252kMOD4g48eje8B2eOdhz9vg/Ol/fq18PLlcO2/oP+Ew/5dtGjXevjiTzDmcqfbqTHdlGiwk2d1crm5uZqXlxfuMLoFn0958L01zPp6C1PGZfPHy48hLqaVOZS2fgNvXAd1e2HyEzDm0uDeqMGdn6e92xTKC2DW+U711LVz27+twOeDWRfAzjVw6yLramq6NBFZrKq5gY5HwOxpplV1lY1m2fQ0+Lj9tWXM+noLN546hEd/Mi5wglCF756F2Rc61S03fRx8ggAnOYSi0Tk1B657F+J7wktToHhV+77+0pdg29fOmAhLEKabsyQRydb9G/7fIfDkCfD1E+wt3cENsxbx7vJC7vrxSO69YBRRUQF6IXmq4Z2b4f1fO4O5fvYZ9BndsfG3Jm2gkyhiEuHFybBzXfu8buUOmP9bGHwajJ/WPq9pTCdmSSJSrf8IXr8Oeo+EhJ7w0T3EPz6aK7fez6yJNdx82pDA3VRLt8DMc5xeRhN/4yyCk5jWkdEHJ2OIkyiiop01GNpjVbcP7nQWrbnwsW65wIwxTVmSiEQbP4XXpjmLlFz3LtsvncsNiY/ziu9HnJe4honf3gh/PdZZ46CyyWR1Gz6Gv53hrGXw09dg4p3OZHOdVeZwp13C54XZF0HJpkN/re8/gNXvOAsCZQ5vvxiN6cSs4TrSbP7CWd0sYyhMf4+15TFcN3MhdQ0+Zk7PZUJ2IqyZC0tehK1fOnP5H3GeMx9Q8Qr45CFnYrorX3KW1ewqile5bScpcP08pzqqLer2OtVy8Snw8y86z1oFxhymgzVcW5KIJNu+hZcuRdMGsPD02byxro55K4vomRDLizcezxF9mqxHsPsHWDIblr3idGcFZ4bTi//aNccFFC5zqp0S02H6PKeBO1gf3A3fPg03fAgDm84uY0zXZUmiG1myrZSU+BiGZaUQHahBOQDdvgjfi1Moi0rnau/9rNubRI/4GM4b05c7zj6C7LTEwBc3eOD7ec7z0ZO7dl18/mKnIbtHH2eZ0B59Wz7P47de8u71zupxuTc4K8cZ041Ykugmlm4r5ZKnvgYgITaK0f16MjYnlaNyUhmbk8rw3inEtrAe9NY9VXz1xadcvOzn7PElM817H6OOHMWU8TmcNbJ3ZCwR2pRboiI1B4ad1WR9ZXfQX32TJSZ79of/83VIl4k0JhwsSXQTM17M47vNJfz2wtGsLixndUEFqwvLqXJnV42LiWJUv56MyXaSR229lznLCqnNX8Ercf9LQ0wSX5/+EhOPmxAZS4MezJYvnZHZ3np3uhC/dZX3TR+S7Lev9yhLEKZbsiTRDWzYWcmPHv2c/5o0gl+efcT+/T6fsnlPFasKyllVUM7KAid5VNY5s7Kek1XK47X3EBefSPQN7ztdQs0Bql276syYdnCwJBHuqcJNEJ5ZsImE2Cimnzy40f6oKGFYljMF9+RxTiOsz6dsK6kmqnQDA/91B8THw/T3LEG0xBKEMQdlSaKTKyyrYc7SAqadOKj1GVhdUQKD69bB3GnO2IDp/7Y+/caYQ2ZJopN7/ovNANx0WislAZ8Xti+Ede85j9ItkJjhjDbuPbJjAjXGdEuWJDqx0ioPryzcxsXjsumfntT4YEMdbFrgJIXv5znLZ0bHwZAz4NQ7YORFkNwrPIEbY7oNSxKd2OxvtlBT7+XmM9yRzXWVzsI6a9+DH+aDpxLiejjrGYy8AEac48zDZIwx7cSSRGdUXUJt0VrKvprL33rv4YiPZzsDusq2gvqc7ppjLnFKC0PPgJj4cEdsjOmmLEmEW2UxrHrLXYLzB9j1PVTvJgF4APBVxUHFEc7COUdfCUMnOqu3RUXgIDhjTIezJBFOniqYfbGz5GZiOmQeCSPPpyFjBHf+p4a69BE8cctkSwjGmLCxJBFO837tlCCmveUs3ON6J287b+1dwQtXHGcJwhgTVp14IYBubtkrsOwfztoEfgnC51OeWbCRkX17MPGIrDAGaIwxliTCY9f38O9fwqBT4Iw7Gx2av3YHG3dVccvEYYFXhjPGmA5iSaKj1dfAG9MhNhEuex6iD9T4qSpP/WcjAzOSuGBsv/DFaIwxLksSHe39O2HnGrjkWeiZ3ejQt5tKWL69jBmnDyWmhWm/jTGmo9k3UUda+aaz0tupd8CIHzU7/PSCjWSmxHP5hP5hCM4YY5qzJNFR9myEd2+HASfAmfc0O7yqoJzP1+/ihlMHR+ZCQMaYTsmSREeor4U3roPoWLh8pvOziWcWbKRHfAzTThwUhgCNMaZlliQ6wkf3QPFKmPI0pDavStqyu4p5K4u4+sRB9EywVeOMMZ2HJYlQWz0HFj0PJ90KR/64xVOe/WITMdFR3HDK4A4NzRhjDsaSRCiVbIa5t0HOBJh0f4un7Kyo5c28fC6f0J/ePRM6OEBjjGldUElCRN4WkQtExJJKsBrq4M3rAYHLX4CY5qvK7ayo5ddvrqDB52PGaUM7PkZjjDmIYL/0nwJ+CvwgIo+IyJEhjKnr83nho3uhcClMeRLSGzdG+3zKy99uZdKjC/hm0x5+e+FoBmcmhylYY4wJLKgJ/lT1Y+BjEUkFprrPtwPPAS+ran0IY+w6ygtg6cuw9CUo3w7H/xxGXdTolPU7Krn77ZUs3lrKSUN78YdLxzLEEoQxppMKehZYEekFTAOuAZYC/wBOBa4DJga45jzgcSAaeF5VH2lyfCAwG0hzz7lLVeeJyGBgLfC9e+q3qnpzsLF2KG8D/PAhLJ4NG+Y7iwINnQhnPwijJ+8/rbbey5OfbeCZBRtJjo/hT1ccw2XH5tj8TMaYTi2oJCEi7wBHAi8BF6lqkXvoNRHJC3BNNPAkcDaQDywSkbmqusbvtHuB11X1aREZDcwDBrvHNqrquDbeT8cp3QJLXnJKDnuLIaWvM5J6/DWQMaTRqd9s3MNv3lnJ5t1VXDI+h3svGEWvFFtNzhjT+QVbkviLqn7W0gFVzQ1wzfHABlXdBCAirwKTAf8kocC+RZlTgcIg4wkPbwOse9cpNWz6DCQKhp8NEx6FEec2mqwPoKzawx/mreX1vHwGZiTx0o3Hc9oIm/7bGNN1BJskRovIUlUtAxCRdGCqqj7VyjU5wHa/7XzghCbnPAB8JCK3AcmA/4RGQ0RkKVAB3KuqXwQZa+j852H44k+QOgAm/gbGX93i4DiAd5cX8sDc1ZTV1HPzGcO4fdIIEuNsug1jTNcSbJL4mao+uW9DVUtF5Gc4vZ4Ox1Rglqr+WUROAl4SkTFAETBQVfeIyARgjogcpaoV/heLyAxgBsDAgQMPM5QgbPwUBpwI189rdcW4z9fv4rZXlnLMgDReumQso7N7BjzXGGM6s2C7wEaLXwur297QvON/YwXAAL/t/u4+fzcCrwOo6jdAApCpqnWqusfdvxjYCBzR9A1U9VlVzVXV3KysEFfjNHhgxyoYcHyrCaLG4+WeOSsZmpnMazNOtARhjOnSgk0SH+A0Uk8SkUnAK+6+1iwCRojIEBGJA64C5jY5ZxswCUBERuEkiV0ikuUmIkRkKDAC2BRkrKGxYxV4PZBzbKunPfbJeraX1PCHS8fabK7GmC4v2OqmO4GfA7e42/OB51u7QFUbRORW4EOc7q0zVXW1iDwI5KnqXOB/gOdE5A6cRuzpqqoicjrwoIjUAz7gZlUtaevNtauCxc7PnAkBT1ldWM7zX2zmytwBnDi0VwcFZowxoRPsYDof8LT7CJqqzsPp1uq/7z6/52uAU1q47i3grba8V8gVLoWkTKfRugVen3L32ytJT4rl7vNHdnBwxhgTGsGOkxgBPAyMxqkSAkBVI2fCoYIlTlVTgMFvs7/ewor8cv4ydTxpSQdrrjHGmK4h2DaJF3BKEQ3AmcCLwMuhCqrTqdsLu7+H7JbbIwrKavjTR98z8cgsLjq6XwcHZ4wxoRNskkhU1U8AUdWtqvoAcEHowupkipY702200Gitqvx2zipU4aHJY2yaDWNMtxJsw3WdO034D25jdAGQErqwOpnCJc7PFkoS81YW8+m6ndx7wSgGZCR1cGDGGBNawZYkbgeSgP8CJuBM9HddqILqdAqWOA3WKY3HYpRX13P/3NWMzUll+smDwxObMcaE0EFLEu54hStV9VfAXuD6kEfV2RQsbrGq6ZEP1lFa7WHW9ccRE23rMRljup+DfrOpqhdnSvDIVLUHyrY2q2pauLmEVxZu48ZThzAmJzVMwRljTGgF2yaxVETmAm8AVft2qurbIYmqMylc6vz0K0nUNXi5++0V9E9P5L9/NCJMgRljTOgFmyQSgD3AWX77FIiAJLEEEOg3bv+up/+zkY27qph1/XEkxQW9bpMxxnQ5wY64jrx2iH0KlkDmCEhwJurbsLOSpz7byORx2Uw8sneYgzPGmNAKdsT1Czglh0ZU9YZ2j6gzUXUarYc5BSifO/VGYlw0v71wdJiDM8aY0Au2ruQ9v+cJwCV09lXk2kNFAVTt3D+p35tL8lm0pZQ/Xn40mbb8qDEmAgRb3dRosj0ReQX4MiQRdSYF7iA6t9H6yx92k52awBUTWl6NzhhjuptD7dw/Auj+FfKFSyAqBvqMAaC4vJb+GUk29YYxJmIE2yZRSeM2iWKcNSa6t4Il0OcoiHUmvi2qqOHYgelhDsoYYzpOsNVNPUIdSKfj80HhMhhzqbupFJfX0i81MbxxGWNMBwqquklELhGRVL/tNBGZErKoOoOSTVBXvr89Yk+Vh3qv0i814SAXGmNM9xFsm8T9qlq+b0NVy4D7QxJRZ9Fk5tfi8loA+lqSMMZEkGCTREvnde+hxgWLITYJspylSAvLawDItuomY0wECTZJ5InIoyIyzH08CiwOZWBhV7AE+h0D0U4utJKEMSYSBZskbgM8wGvAq0At8ItQBRV23nooXtFo5tei8lpio4VeybZ+tTEmcgTbu6kKuCvEsXQeO9dCQ22jmV+Lymvom5pAVJSNkTDGRI5gezfNF5E0v+10EfkwZFGF2/5G6/H7dxWV19Kvp7VHGGMiS7DVTZlujyYAVLWU7jziumAJJKRBxtD9u4rLa609whgTcYJNEj4RGbhvQ0QG08KssN1G4RKnFOFOv6G6byCdJQljTGQJthvrPcCXIrIAEOA0YEbIogonTzXsWAOn3rF/154qDx6vz5KEMSbiBNtw/YGI5OIkhqXAHKAmhHGFT/FKUG+jRusD3V+tTcIYE1mCneDvJuB2oD+wDDgR+IbGy5l2D01GWoPTaA1YScIYE3GCbZO4HTgO2KqqZwLjgbJQBRVWBUugRz/o2W//riJ3tHW/NEsSxpjIEmySqFXVWgARiVfVdcCRoQsrjAqXNCpFgFOSiIkSMpNtNTpjTGQJtuE63x0nMQeYLyKlwNZQBRU2NWWwZwMcc1Wj3cXltfTpaQPpjDGRJ9iG60vcpw+IyGdAKvBByKIKl8Klzk93Tev9u8tqyLaqJmNMBGrzTK6quiAUgXQKLYy0BiiuqOXo/mkdH48xxoTZoa5xHRQROU9EvheRDSLSbO4nERkoIp+JyFIRWSEi5/sdu9u97nsROTeUce5XsMQZZZ14YIlSVXWm5LCeTcaYCBSyJCEi0cCTwI+B0cBUERnd5LR7gddVdTxwFfCUe+1od/so4DzgKff1QqtwabNG65IqD54GG0hnjIlMoSxJHA9sUNVNqurBmWJ8cpNzFOjpPk8FCt3nk4FXVbVOVTcDG9zXC53KHVBR0GgQHdgYCWNMZAtlksgBtvtt57v7/D0ATBORfGAezroVwV6LiMwQkTwRydu1a9fhRdvCIDqw0dbGmMgW0jaJIEwFZqlqf+B84CURCTomVX1WVXNVNTcrK+vwIilYAhIF/Y5utLto/7KlVpIwxkSeUK5TXQAM8Nvu7+7zdyNOmwOq+o2IJACZQV7bvgoWQ+/REJfcaPe+gXS9UmwgnTEm8oSyJLEIGCEiQ0QkDqchem6Tc7YBkwBEZBSQAOxyz7tKROJFZAgwAlgYskhVD0wP3sS+gXTRNpDOGBOBQlaSUNUGEbkV+BCIBmaq6moReRDIU9W5wP8Az4nIHTiN2NNVVYHVIvI6sAZoAH6hqt5QxUrpFqgpbdZoDU5JwhYbMsZEqlBWN6Gq83AapP333ef3fA1wSoBrfw/8PpTx7Reg0RqcNokxOakdEoYxxnQ24W647hwKlkB0PPQ5qtFuG0hnjIl0liTAGUTXdyxExzbaXVZdT12Dz7q/GmMiliUJnxcKl7XYHlFo3V+NMRHOkkRlEcT3aDbzK/gPpLMkYYyJTCFtuO4SUvvDr74Hn6/ZoQNTclh1kzEmMllJYp+o5r+KovIaoqOErB42kM4YE5ksSbSiqLyWPj3ibSCdMSZiWZJoRbENpDPGRDhLEq0oKq+lX5q1RxhjIpcliQCcgXQ19OtpJQljTOSyJBFAeU09tfU+q24yxkQ0SxIBFJY53V+zrbrJGBPBLEkEUFzhjLa2koQxJpJZkgjA1rY2xhhLEgEVl9cSJZBlK9IZYyKYJYkACsucFeliou1XZIyJXPYNGEBxRY21RxhjIp4liQBssSFjjLEk0SJVpais1mZ/NcZEPEsSLaioaaCm3mslCWNMxLMk0YIiGyNhjDGAJYkWFZXZYkPGGAOWJFpkA+mMMcZhSaIFxeU1RAn0thXpjDERzpJECwrLa+ndwwbSGWOMfQu2wFakM8YYhyWJFhSV11h7hDHGYEmiGWdFOhtIZ4wxYEmimYraBqo9NpDOGGPAkkQzxW73V2uTMMYYSxLNFJU7o62tJGGMMZYkmtk/kM7WtjbGmNAmCRE5T0S+F5ENInJXC8f/PxFZ5j7Wi0iZ3zGv37G5oYzTX1F5LWID6YwxBoCYUL2wiEQDTwJnA/nAIhGZq6pr9p2jqnf4nX8bMN7vJWpUdVyo4gukuLyGrJR4Ym0gnTHGhLQkcTywQVU3qaoHeBWY3Mr5U4FXQhhPUIrKa62qyRhjXKFMEjnAdr/tfHdfMyIyCBgCfOq3O0FE8kTkWxGZEuC6Ge45ebt27WqXoIvKa+nX0xqtjTEGOk/D9VXAm6rq9ds3SFVzgZ8Cj4nIsKYXqeqzqpqrqrlZWVntEohNyWGMMQeEMkkUAAP8tvu7+1pyFU2qmlS1wP25CfgPjdsrQqKitp69dQ1kp1mSMMYYCG2SWASMEJEhIhKHkwia9VISkZFAOvCN3750EYl3n2cCpwBrml7b3g4MpLM2CWOMgRD2blLVBhG5FfgQiAZmqupqEXkQyFPVfQnjKuBVVVW/y0cBfxMRH04ie8S/V1So2GJDxhjTWMiSBICqzgPmNdl3X5PtB1q47mtgbChja0lRmY22NsYYf52l4bpTODCQzpKEMcaAJYlGistryUyJJy7Gfi3GGAOWJBopLK8h26qajDFmP0sSfmyMhDHGNGZJwk+xrUhnjDGNWJJwVdbWU1nXYCUJY4zxY0nCVWxjJIwxphlLEq4DA+msuskYY/axJOGykoQxxjRnScJV6K5t3cemCTfGmP0sSbhsIJ0xxjRn34iuovJaq2oyxpgmLEm4isprLEkYY0wTliRcVpIwxpjmLEkAe+saqKxtsMWGjDGmCUsSQLHbs8mWLTXGmMYsSXBgIF1f6/5qjDGNWJLARlsbY0wgliSAojInSfRJjQ9zJMYY07lYkgCKK2rITIkjPiY63KEYY0ynYkkCp7rJpgg3xpjmLEngVDf17WntEcYY05QlCZzR1tb91Rhjmov4JFFV10BFra1IZ4wxLYn4JFHX4OOiY7IZk50a7lCMMabTiQl3AOGWkRzHX6eOD3cYxhjTKUV8ScIYY0xgliSMMcYEZEnCGGNMQJYkjDHGBGRJwhhjTECWJIwxxgRkScIYY0xAliSMMcYEJKoa7hjahYjsArYexktkArvbKZzOoLvdD3S/e+pu9wPd75662/1A83sapKpZgU7uNknicIlInqrmhjuO9tLd7ge63z11t/uB7ndP3e1+oO33ZNVNxhhjArIkYYwxJiBLEgc8G+4A2ll3ux/ofvfU3e4Hut89dbf7gTbek7VJGGOMCchKEsYYYwKyJGGMMSagiE8SInKeiHwvIhtE5K5wx9MeRGSLiKwUkWUikhfueNpKRGaKyE4RWeW3L0NE5ovID+7P9HDG2FYB7ukBESlwP6dlInJ+OGNsCxEZICKficgaEVktIre7+7vk59TK/XTlzyhBRBaKyHL3nn7n7h8iIt+533mviUhcq68TyW0SIhINrAfOBvKBRcBUVV0T1sAOk4hsAXJVtUsOAhKR04G9wIuqOsbd90egRFUfcZN5uqreGc442yLAPT0A7FXVP4UztkMhIv2Afqq6RER6AIuBKcB0uuDn1Mr9/ISu+xkJkKyqe0UkFvgSuB34JfC2qr4qIs8Ay1X16UCvE+klieOBDaq6SVU9wKvA5DDHFPFU9XOgpMnuycBs9/lsnP/AXUaAe+qyVLVIVZe4zyuBtUAOXfRzauV+uix17HU3Y92HAmcBb7r7D/oZRXqSyAG2+23n08X/YbgU+EhEFovIjHAH0076qGqR+7wY6BPOYNrRrSKywq2O6hJVM02JyGBgPPAd3eBzanI/0IU/IxGJFpFlwE5gPrARKFPVBveUg37nRXqS6K5OVdVjgR8Dv3CrOroNdepIu0M96dPAMGAcUAT8OazRHAIRSQHeAv5bVSv8j3XFz6mF++nSn5GqelV1HNAfp+ZkZFtfI9KTRAEwwG+7v7uvS1PVAvfnTuAdnH8cXd0Ot954X/3xzjDHc9hUdYf7n9gHPEcX+5zceu63gH+o6tvu7i77ObV0P139M9pHVcuAz4CTgDQRiXEPHfQ7L9KTxCJghNvaHwdcBcwNc0yHRUSS3YY3RCQZOAdY1fpVXcJc4Dr3+XXAv8IYS7vY92XquoQu9Dm5jaJ/B9aq6qN+h7rk5xTofrr4Z5QlImnu80ScDjprcZLF5e5pB/2MIrp3E4Dbpe0xIBqYqaq/D29Eh0dEhuKUHgBigH92tXsSkVeAiThTGu8A7gfmAK8DA3GmhP+JqnaZhuAA9zQRpxpDgS3Az/3q8zs1ETkV+AJYCfjc3b/Bqcfvcp9TK/czla77GR2N0zAdjVMgeF1VH3S/I14FMoClwDRVrQv4OpGeJIwxxgQW6dVNxhhjWmFJwhhjTECWJIwxxgRkScIYY0xAliSMMcYEZEnCmDYQEa/fjKDL2nPmYBEZ7D9LrDGdQczBTzHG+KlxpzkwJiJYScKYduCu4fFHdx2PhSIy3N0/WEQ+dSeI+0REBrr7+4jIO+5c/8tF5GT3paJF5Dl3/v+P3JGyxoSNJQlj2iaxSXXTlX7HylV1LPAEzih+gL8Cs1X1aOAfwF/c/X8BFqjqMcCxwGp3/wjgSVU9CigDLgvp3RhzEDbi2pg2EJG9qprSwv4twFmqusmdKK5YVXuJyG6cxWzq3f1FqpopIruA/v7TIbhTVM9X1RHu9p1ArKr+bwfcmjEtspKEMe1HAzxvC/85dLxYu6EJM0sSxrSfK/1+fuM+/xpndmGAq3EmkQP4BLgF9i8Mk9pRQRrTFvZXijFtk+iu9LXPB6q6rxtsuoiswCkNTHX33Qa8ICK/BnYB17v7bweeFZEbcUoMt+AsamNMp2JtEsa0A7dNIldVd4c7FmPak1U3GWOMCchKEsYYYwKykoQxxpiALEkYY4wJyJKEMcaYgCxJGGOMCciShDHGmID+f72QigtoRZMIAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# SOLUTION\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='training')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91BgOsDnCa4W"
      },
      "source": [
        "By unfreezing the last convolutional layers of the pre-trained VGG16 and training the model for 50 more epochs, we achieved even better accuracy. We have now 0.89 on the testing set.\n",
        "\n",
        "### Discussion: What are the reasons why fine-tuning helped to improve the accuracy score."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DL_Lab4_Exercise2_Solutions.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "5826022c13cf670f25f82c54601002af42321e6e77593e3891ed52ffe2de205c"
    },
    "kernelspec": {
      "display_name": "Python 3.8.1 ('azureml_py38_tensorflow')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
